{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/deeplearning/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Add, merge, concatenate\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.pooling import MaxPooling3D, GlobalAveragePooling3D, AveragePooling3D\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import he_uniform\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(\"models/\")\n",
    "sys.path.append(\"scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4, 5, 6, 7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_classes import DataGenerator, AugmentedDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3011, 24, 24, 24, 16) (377, 24, 24, 24, 16) (376, 24, 24, 24, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "h5f = h5py.File('data/data_cano_distributed2.h5', 'r')\n",
    "train_x, train_y = h5f['train_x'][:], h5f['train_y'][:]\n",
    "valid_x, valid_y = h5f['valid_x'][:], h5f['valid_y'][:]\n",
    "test_x, test_y = h5f['test_x'][:], h5f['test_y'][:]\n",
    "h5f.close()\n",
    "\n",
    "print(train_x.shape, valid_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Squeeze_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "# model_input = Input(shape=(24, 24, 24, 16))\n",
    "# squeeze_model = Model(inputs=model_input, outputs=Squeeze_model(model_input))\n",
    "# #plot_model(squeeze_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# SVG(model_to_dot(squeeze_model, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))\n",
    "# # plot_model(get_model4((24, 24, 24, 16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "nb_gpus = 4\n",
    "nb_batch = nb_gpus*3\n",
    "nb_epochs =200\n",
    "l_rate = 1e-5\n",
    "decay_rate = l_rate / nb_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "outputFolder = './weights'\n",
    "# if not os.path.exists(outputFolder):\n",
    "#     os.makedirs(outputFolder)\n",
    "\n",
    "filepath=outputFolder+\"/weights-distributed_cano_rotated_90.h5\"\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath, \n",
    "                                  monitor='val_loss',\n",
    "                                  verbose=1,\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=True,\n",
    "                                  mode='auto', period=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(24, 24, 24, 16))\n",
    "squeeze_model = Model(inputs=model_input, outputs=Squeeze_model(model_input))\n",
    "model = multi_gpu_model(squeeze_model, gpus=nb_gpus)\n",
    "\n",
    "model.compile(optimizer=optimizers.adam(lr=l_rate),# beta_1=0.99, beta_2=0.999),\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_gen = AugmentedDataGenerator(x=train_x, y=train_y, batch_size=nb_batch)\n",
    "aug_val_gen = AugmentedDataGenerator(x=valid_x, y=valid_y, batch_size=nb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 24, 24, 24, 16) (288,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in aug_data_gen:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "250/250 [==============================] - 319s 1s/step - loss: 2.7923 - val_loss: 1.4809\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48086, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 249s 994ms/step - loss: 1.5910 - val_loss: 1.6245\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 356s 1s/step - loss: 1.5970 - val_loss: 1.5107\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 284s 1s/step - loss: 1.6165 - val_loss: 1.7235\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 361s 1s/step - loss: 1.5458 - val_loss: 1.6098\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 360s 1s/step - loss: 1.5568 - val_loss: 1.5188\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 295s 1s/step - loss: 1.5377 - val_loss: 1.4921\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 391s 2s/step - loss: 1.5586 - val_loss: 1.5290\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 291s 1s/step - loss: 1.5076 - val_loss: 1.6038\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 301s 1s/step - loss: 1.5473 - val_loss: 1.5626\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 162s 649ms/step - loss: 1.5433 - val_loss: 1.4620\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.48086 to 1.46203, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 159s 636ms/step - loss: 1.5223 - val_loss: 1.4297\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.46203 to 1.42971, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 164s 655ms/step - loss: 1.4819 - val_loss: 1.4486\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 189s 756ms/step - loss: 1.4660 - val_loss: 1.4256\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.42971 to 1.42565, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 233s 933ms/step - loss: 1.5248 - val_loss: 1.4977\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 1.5246 - val_loss: 1.3990\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.42565 to 1.39905, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 166s 664ms/step - loss: 1.5188 - val_loss: 1.5656\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 164s 656ms/step - loss: 1.4721 - val_loss: 1.5068\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 163s 653ms/step - loss: 1.4897 - val_loss: 1.4937\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 162s 650ms/step - loss: 1.4529 - val_loss: 1.4642\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 164s 654ms/step - loss: 1.4969 - val_loss: 1.4769\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 161s 646ms/step - loss: 1.4832 - val_loss: 1.3904\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.39905 to 1.39044, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 163s 651ms/step - loss: 1.5061 - val_loss: 1.4287\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 163s 651ms/step - loss: 1.4540 - val_loss: 1.4994\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 163s 651ms/step - loss: 1.4325 - val_loss: 1.4217\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 166s 665ms/step - loss: 1.4942 - val_loss: 1.3948\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 162s 646ms/step - loss: 1.4885 - val_loss: 1.4145\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 163s 653ms/step - loss: 1.5115 - val_loss: 1.3977\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 162s 650ms/step - loss: 1.4886 - val_loss: 1.6895\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 162s 648ms/step - loss: 1.4563 - val_loss: 1.4726\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 159s 634ms/step - loss: 1.4842 - val_loss: 1.4410\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 148s 592ms/step - loss: 1.4632 - val_loss: 1.6398\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.4616 - val_loss: 1.5091\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 144s 577ms/step - loss: 1.4739 - val_loss: 1.4078\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 146s 583ms/step - loss: 1.4545 - val_loss: 1.3927\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 149s 595ms/step - loss: 1.4200 - val_loss: 1.4638\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.4390 - val_loss: 1.4493\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 146s 583ms/step - loss: 1.4745 - val_loss: 1.6445\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 147s 588ms/step - loss: 1.4397 - val_loss: 1.4657\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 143s 571ms/step - loss: 1.4768 - val_loss: 1.4192\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 147s 588ms/step - loss: 1.4506 - val_loss: 1.4618\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 149s 597ms/step - loss: 1.4740 - val_loss: 1.4017\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 144s 578ms/step - loss: 1.4199 - val_loss: 1.5723\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 147s 589ms/step - loss: 1.4344 - val_loss: 1.4480\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 146s 583ms/step - loss: 1.4591 - val_loss: 1.4418\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 146s 583ms/step - loss: 1.4046 - val_loss: 1.4015\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.4262 - val_loss: 1.3232\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.39044 to 1.32315, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.4117 - val_loss: 1.3680\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.4581 - val_loss: 1.4934\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 147s 586ms/step - loss: 1.4071 - val_loss: 1.4077\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 146s 583ms/step - loss: 1.4795 - val_loss: 1.5112\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.4388 - val_loss: 1.3154\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.32315 to 1.31545, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 146s 582ms/step - loss: 1.4341 - val_loss: 1.4705\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.3900 - val_loss: 1.3280\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.4194 - val_loss: 1.3305\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 148s 591ms/step - loss: 1.4203 - val_loss: 1.3893\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.3848 - val_loss: 1.3951\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 148s 592ms/step - loss: 1.4176 - val_loss: 1.6851\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.4108 - val_loss: 1.3747\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 145s 578ms/step - loss: 1.3910 - val_loss: 1.3588\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 145s 582ms/step - loss: 1.4068 - val_loss: 1.3203\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 147s 586ms/step - loss: 1.3861 - val_loss: 1.3514\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 143s 571ms/step - loss: 1.4439 - val_loss: 1.3279\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 147s 589ms/step - loss: 1.4081 - val_loss: 1.4227\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 144s 577ms/step - loss: 1.3907 - val_loss: 1.3339\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 150s 602ms/step - loss: 1.3925 - val_loss: 1.2864\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.31545 to 1.28637, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 143s 571ms/step - loss: 1.4103 - val_loss: 1.4360\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 144s 577ms/step - loss: 1.3785 - val_loss: 1.3038\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 144s 576ms/step - loss: 1.3827 - val_loss: 1.3374\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.4095 - val_loss: 1.3491\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.3908 - val_loss: 1.4259\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 145s 578ms/step - loss: 1.3898 - val_loss: 1.3689\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 147s 586ms/step - loss: 1.3817 - val_loss: 1.4593\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 147s 587ms/step - loss: 1.3911 - val_loss: 1.3020\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 147s 587ms/step - loss: 1.3775 - val_loss: 1.2855\n",
      "\n",
      "Epoch 00075: val_loss improved from 1.28637 to 1.28551, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.4000 - val_loss: 1.2719\n",
      "\n",
      "Epoch 00076: val_loss improved from 1.28551 to 1.27188, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 147s 589ms/step - loss: 1.3754 - val_loss: 1.3656\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 145s 582ms/step - loss: 1.3557 - val_loss: 1.3229\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 147s 587ms/step - loss: 1.3712 - val_loss: 1.3763\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 147s 586ms/step - loss: 1.3498 - val_loss: 1.3363\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 148s 592ms/step - loss: 1.3780 - val_loss: 1.4944\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 146s 582ms/step - loss: 1.3738 - val_loss: 1.3818\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 142s 568ms/step - loss: 1.3867 - val_loss: 1.3987\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 146s 585ms/step - loss: 1.3407 - val_loss: 1.3326\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 148s 590ms/step - loss: 1.3568 - val_loss: 1.3390\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 147s 589ms/step - loss: 1.3772 - val_loss: 1.3768\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.3657 - val_loss: 1.3332\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 147s 588ms/step - loss: 1.3710 - val_loss: 1.3120\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 144s 577ms/step - loss: 1.3986 - val_loss: 1.2390\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.27188 to 1.23898, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 147s 588ms/step - loss: 1.3934 - val_loss: 1.3663\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 145s 580ms/step - loss: 1.3673 - val_loss: 1.3902\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 146s 582ms/step - loss: 1.3474 - val_loss: 1.4215\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.3632 - val_loss: 1.3415\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 148s 593ms/step - loss: 1.3597 - val_loss: 1.3312\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 146s 584ms/step - loss: 1.3536 - val_loss: 1.2492\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 148s 593ms/step - loss: 1.3530 - val_loss: 1.3210\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 147s 588ms/step - loss: 1.3946 - val_loss: 1.3998\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 148s 590ms/step - loss: 1.3077 - val_loss: 1.2731\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 147s 589ms/step - loss: 1.3818 - val_loss: 1.3189\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 148s 591ms/step - loss: 1.3875 - val_loss: 1.2729\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 145s 578ms/step - loss: 1.3727 - val_loss: 1.2696\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 145s 581ms/step - loss: 1.3413 - val_loss: 1.3525\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 144s 575ms/step - loss: 1.3504 - val_loss: 1.3562\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 144s 576ms/step - loss: 1.3278 - val_loss: 1.3387\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.3307 - val_loss: 1.3060\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 138s 552ms/step - loss: 1.3617 - val_loss: 1.3381\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 135s 541ms/step - loss: 1.3531 - val_loss: 1.3336\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 135s 540ms/step - loss: 1.3495 - val_loss: 1.2844\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 135s 540ms/step - loss: 1.3582 - val_loss: 1.4180\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 133s 530ms/step - loss: 1.3084 - val_loss: 1.2353\n",
      "\n",
      "Epoch 00110: val_loss improved from 1.23898 to 1.23532, saving model to ./weights/weights-distributed_cano_rotated_90.h5\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.3573 - val_loss: 1.3581\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.2982 - val_loss: 1.2510\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.3644 - val_loss: 1.2703\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.3448 - val_loss: 1.3448\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.3392 - val_loss: 1.3464\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 1.3141 - val_loss: 1.3728\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 133s 530ms/step - loss: 1.3462 - val_loss: 1.3746\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 1.3374 - val_loss: 1.3346\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.3424 - val_loss: 1.3249\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.3550 - val_loss: 1.2895\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.3156 - val_loss: 1.3261\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 133s 531ms/step - loss: 1.2915 - val_loss: 1.2798\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.3477 - val_loss: 1.3094\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 133s 534ms/step - loss: 1.3094 - val_loss: 1.3235\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.3086 - val_loss: 1.3258\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.3035 - val_loss: 1.3983\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.3152 - val_loss: 1.3504\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.2960 - val_loss: 1.3614\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 1.3049 - val_loss: 1.4107\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.3137 - val_loss: 1.3644\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.3069 - val_loss: 1.2748\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.3122 - val_loss: 1.3619\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.3120 - val_loss: 1.3376\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 135s 538ms/step - loss: 1.2933 - val_loss: 1.3067\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 136s 542ms/step - loss: 1.2958 - val_loss: 1.2369\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.2942 - val_loss: 1.4087\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.3012 - val_loss: 1.3412\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.2744 - val_loss: 1.3323\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 131s 522ms/step - loss: 1.2954 - val_loss: 1.3508\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.3005 - val_loss: 1.2631\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 132s 529ms/step - loss: 1.3102 - val_loss: 1.2967\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.2902 - val_loss: 1.2514\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.2413 - val_loss: 1.3221\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.3091 - val_loss: 1.3387\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 135s 538ms/step - loss: 1.2520 - val_loss: 1.3237\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.2893 - val_loss: 1.4108\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 133s 532ms/step - loss: 1.2798 - val_loss: 1.2952\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 134s 535ms/step - loss: 1.2896 - val_loss: 1.3695\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.2905 - val_loss: 1.3673\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 133s 532ms/step - loss: 1.2798 - val_loss: 1.2958\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.2823 - val_loss: 1.3171\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 1.2762 - val_loss: 1.4155\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.2923 - val_loss: 1.3397\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 135s 538ms/step - loss: 1.2698 - val_loss: 1.3514\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.2623 - val_loss: 1.3173\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 137s 549ms/step - loss: 1.2657 - val_loss: 1.3838\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 134s 534ms/step - loss: 1.2819 - val_loss: 1.3387\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.2768 - val_loss: 1.2998\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 131s 525ms/step - loss: 1.3002 - val_loss: 1.3378\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.2427 - val_loss: 1.3278\n",
      "\n",
      "Epoch 00160: val_loss did not improve\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.2722 - val_loss: 1.2984\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.2715 - val_loss: 1.3035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.2369 - val_loss: 1.3137\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 134s 535ms/step - loss: 1.2438 - val_loss: 1.4203\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 133s 532ms/step - loss: 1.2672 - val_loss: 1.3310\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 134s 538ms/step - loss: 1.2647 - val_loss: 1.3132\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 133s 533ms/step - loss: 1.2346 - val_loss: 1.2682\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.2507 - val_loss: 1.3447\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.2411 - val_loss: 1.3514\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 135s 540ms/step - loss: 1.2303 - val_loss: 1.3528\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 137s 548ms/step - loss: 1.2447 - val_loss: 1.3904\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.2328 - val_loss: 1.2768\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 137s 549ms/step - loss: 1.2151 - val_loss: 1.2978\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.2112 - val_loss: 1.4129\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 132s 528ms/step - loss: 1.2255 - val_loss: 1.2843\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.1933 - val_loss: 1.3033\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.2253 - val_loss: 1.2629\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.2214 - val_loss: 1.2981\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 134s 538ms/step - loss: 1.2301 - val_loss: 1.3233\n",
      "\n",
      "Epoch 00179: val_loss did not improve\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 133s 534ms/step - loss: 1.2104 - val_loss: 1.4129\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 1.2234 - val_loss: 1.4746\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.1933 - val_loss: 1.2908\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.2319 - val_loss: 1.3624\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 135s 542ms/step - loss: 1.2125 - val_loss: 1.3289\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 137s 548ms/step - loss: 1.1999 - val_loss: 1.3474\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 137s 546ms/step - loss: 1.2075 - val_loss: 1.2644\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.1804 - val_loss: 1.3271\n",
      "\n",
      "Epoch 00187: val_loss did not improve\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.1831 - val_loss: 1.2855\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.1936 - val_loss: 1.2859\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 137s 546ms/step - loss: 1.1447 - val_loss: 1.3321\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.1774 - val_loss: 1.2442\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      "Epoch 192/200\n",
      "250/250 [==============================] - 135s 538ms/step - loss: 1.1684 - val_loss: 1.4229\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.1817 - val_loss: 1.4555\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 137s 546ms/step - loss: 1.1394 - val_loss: 1.3686\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 1.1334 - val_loss: 1.3655\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 135s 542ms/step - loss: 1.1769 - val_loss: 1.3252\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.1854 - val_loss: 1.3606\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 134s 536ms/step - loss: 1.1446 - val_loss: 1.3671\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 131s 526ms/step - loss: 1.1281 - val_loss: 1.2674\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 137s 549ms/step - loss: 1.1414 - val_loss: 1.4144\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=aug_data_gen, validation_data=aug_val_gen,\n",
    "                              use_multiprocessing=False, \n",
    "                              epochs=nb_epochs, \n",
    "                              max_queue_size=5, \n",
    "                              workers=56, \n",
    "                              verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(outputFolder, \"history_distributed_cano_rotated_90.pickle\"), 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First 100 epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/weights_distributed_cano_rotated_90.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2 (without average):  0.3259518022550295\n",
      "Train r2 (with average):  0.3385686170203386\n",
      "Test r2 (without average):  0.308429803951493\n",
      "Test r2 (with average):  0.323514774352037\n"
     ]
    }
   ],
   "source": [
    "sample_size = 200\n",
    "rotation_count = 24\n",
    "\n",
    "train_aug_data_gen = AugmentedDataGenerator(x=train_x,\n",
    "                                            y=train_y,\n",
    "                                            batch_size=sample_size)\n",
    "# Two chunks to avoid memory issues\n",
    "test_aug_data_gen= AugmentedDataGenerator(x=test_x[:sample_size],\n",
    "                                            y=test_y[:sample_size],\n",
    "                                            batch_size=sample_size)\n",
    "\n",
    "# Train r2\n",
    "\n",
    "for x, y in train_aug_data_gen:\n",
    "    # Without average\n",
    "    train_r2 = r2_score(y_true=y, y_pred=model.predict(x))\n",
    "    print(\"Train r2 (without average): \", train_r2)\n",
    "    \n",
    "    # With average\n",
    "    y_pred = model.predict(x)[:, 0]\n",
    "    \n",
    "    sample_y = np.zeros(sample_size)\n",
    "    sample_ypred = np.zeros(sample_size)\n",
    "    for i in range(sample_size):\n",
    "        start = i*rotation_count\n",
    "        end = i*rotation_count + rotation_count\n",
    "        mean_ypred = np.mean(y_pred[start:end])\n",
    "        mean_y = np.mean(y[start:end])\n",
    "        sample_ypred[i] = mean_ypred\n",
    "        sample_y[i] = mean_y\n",
    "    \n",
    "    train_r2 = r2_score(y_true=sample_y, y_pred=sample_ypred)\n",
    "    print(\"Train r2 (with average): \", train_r2)\n",
    "    break\n",
    "    \n",
    "#sample_size = test_x.shape[0]\n",
    "    \n",
    "# Test r2\n",
    "\n",
    "for x, y in test_aug_data_gen:\n",
    "    # Without average\n",
    "    test_r2 = r2_score(y_true=y, y_pred=model.predict(x))\n",
    "    print(\"Test r2 (without average): \", test_r2)\n",
    "    \n",
    "    # With average\n",
    "    y_pred = model.predict(x)[:, 0]\n",
    "    \n",
    "    sample_y = np.zeros(sample_size)\n",
    "    sample_ypred = np.zeros(sample_size)\n",
    "    for i in range(sample_size):\n",
    "        start = i*rotation_count\n",
    "        end = i*rotation_count + rotation_count\n",
    "        mean_ypred = np.mean(y_pred[start:end])\n",
    "        mean_y = np.mean(y[start:end])\n",
    "        sample_ypred[i] = mean_ypred\n",
    "        sample_y[i] = mean_y\n",
    "    \n",
    "    test_r2 = r2_score(y_true=sample_y, y_pred=sample_ypred)\n",
    "    print(\"Test r2 (with average): \", test_r2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
