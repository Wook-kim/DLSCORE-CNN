{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HTMD License accepted automatically. Check license here: https://raw.githubusercontent.com/Acellera/htmd/master/htmd/LICENCE.txt\n",
      "\n",
      "For advanced features (e.g. parameterize) and to remove this message, we recommend registering. Run htmd_register in your terminal.\n",
      "\n",
      "Please cite HTMD: Doerr et al.(2016)JCTC,12,1845. \n",
      "https://dx.doi.org/10.1021/acs.jctc.6b00049\n",
      "Documentation: http://software.acellera.com/\n",
      "To update: conda update htmd -c acellera -c psi4\n",
      "\n",
      "You are on the latest HTMD version (1.12.3).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.pooling import MaxPooling3D, GlobalAveragePooling3D, AveragePooling3D\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import he_uniform\n",
    "import htmd.ui as ht\n",
    "import htmd.molecule.voxeldescriptors as vd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import bcolz as bc\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 3353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(length, valid_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    INPUT: length of the data\n",
    "    OUTPUT: Indices of the data.\n",
    "    \"\"\"\n",
    "    # Generate indices\n",
    "    indices = [i for i in range(length)]\n",
    "    # Get required number of test and validation indices\n",
    "    test_count = int(length*test_size)\n",
    "    valid_count = int(length*valid_size)\n",
    "    # Choose random test indices\n",
    "    test_ids = list(np.random.choice(indices, test_count))\n",
    "    # Remove test ids from indices\n",
    "    indices = list(set(indices) - set(test_ids))\n",
    "    # Choose random validation indices\n",
    "    valid_ids = list(np.random.choice(indices, valid_count))\n",
    "    # Remove valid ids from indices\n",
    "    train_ids = list(set(indices) - set(valid_ids))\n",
    "    \n",
    "    return train_ids, valid_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file and get all the pdb ids\n",
    "def read_score():\n",
    "    pdb_ids = []\n",
    "    score = {}\n",
    "    with open('pdbbind_refined_set.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None) # Skip the header\n",
    "        for row in reader:\n",
    "            pdb_ids.append(row[1])\n",
    "            score[row[1]]= float(row[5])\n",
    "\n",
    "    return pdb_ids, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, scores = read_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...Done\n",
      "Feature dictionary length:  3971\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "filepath = \"features_dict.pickle\"\n",
    "if os.path.isfile(filepath):\n",
    "    print(\"Loading features...\", end='')\n",
    "    with open(filepath, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Extracting features\", end='')\n",
    "    # Get the pocket files\n",
    "    data_dir = \"../../pdbbind_data/\"\n",
    "    files = glob.glob(data_dir + \"*/*/*_pocket.pdb\", recursive=True)\n",
    "    \n",
    "    pbar = tqdm_notebook(total=len(files))\n",
    "    for i, file in enumerate(files):\n",
    "        m = ht.Molecule(file)\n",
    "        _id = m.viewname[:4]\n",
    "        try:\n",
    "            f, centers, natoms = vd.getVoxelDescriptors(m)\n",
    "            f = f.reshape(natoms[0], natoms[1], natoms[2], -1)\n",
    "            features[_id] = f\n",
    "        except:\n",
    "            continue\n",
    "        pbar.update()\n",
    "    with open(\"features_dict.pickle\", \"wb\") as f:\n",
    "        pickle.dump(features, f)\n",
    "    print(\"Done\")\n",
    "\n",
    "print(\"Feature dictionary length: \", len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "for i, k in enumerate(features.keys()):\n",
    "    shapes.append(features[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 45, 46,  8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_shape = np.max(shapes, axis=0)\n",
    "max_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 17, 17,  8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_shape = np.min(shapes, axis=0)\n",
    "min_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to 50 voxel size (padding with zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reshaped features ' bc_features ' from the disk\n",
      "Reshaped feature shape:  (3971, 50, 50, 50, 8)  and number of pdb ids:  3971\n"
     ]
    }
   ],
   "source": [
    "bc_path = 'bc_features'\n",
    "pdb_ids_file = 'pdb_ids_list'\n",
    "pdb_ids = []\n",
    "\n",
    "# Load the pdb_ids if available\n",
    "if os.path.isfile(pdb_ids_file):\n",
    "    with open(pdb_ids_file, 'rb') as f:\n",
    "        pdb_ids = pickle.load(f)\n",
    "\n",
    "# Load the reshaped features if available\n",
    "if os.path.isdir(bc_path):\n",
    "    print(\"Loading reshaped features '\", bc_path, \"' from the disk\")\n",
    "    re_features = bc.open(bc_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Reshaping data and saving to the disk as '\", bc_path, \"'\")\n",
    "    voxel_size = 50\n",
    "    n_channels = 8\n",
    "    re_features = np.zeros([len(features), voxel_size, voxel_size, voxel_size, n_channels])\n",
    "    pbar = tqdm_notebook(total=len(features))\n",
    "    for i, (k, f) in enumerate(features.items()):\n",
    "        pdb_ids.append(k)\n",
    "        dim_diff = (re_features.shape[1:] - np.array(f.shape)).astype(int)\n",
    "        pad_dim = np.round(dim_diff / 2).astype(int)\n",
    "        f = np.pad(f, [(pad_dim[0], dim_diff[0]-pad_dim[0]),\n",
    "                       (pad_dim[1], dim_diff[1]-pad_dim[1]),\n",
    "                       (pad_dim[2], dim_diff[2]-pad_dim[2]),\n",
    "                       (0, 0)],\n",
    "                   'constant')\n",
    "        re_features[i] = f\n",
    "        pbar.update()\n",
    "    \n",
    "    # Create a bc array   \n",
    "    bc_features = bc.carray(re_features, rootdir=bc_path)\n",
    "    # Write to the disk\n",
    "    bc_features.flush()\n",
    "    # Write the pdb ids\n",
    "    with open(pdb_ids_file, 'wb') as f:\n",
    "        pickle.dump(pdb_ids, f)\n",
    "        \n",
    "print(\"Reshaped feature shape: \", re_features.shape, \" and number of pdb ids: \", len(pdb_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter and create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x data:  (3735, 50, 50, 50, 8)  and shape of y_data:  (3735,)\n"
     ]
    }
   ],
   "source": [
    "x_path = \"bc_x_data\"\n",
    "y_path = \"y_data.pickle\"\n",
    "\n",
    "if os.path.isfile(y_path):\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_data = pickle.load(f)\n",
    "\n",
    "if os.path.isdir(x_path):\n",
    "    x_data = bc.open(x_path)\n",
    "else:\n",
    "    x_data = np.zeros(re_features.shape)\n",
    "    y_data = np.zeros(re_features.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    pbar = tqdm_notebook(total=re_features.shape[0])\n",
    "    for i, _id in enumerate(pdb_ids):\n",
    "        if _id in scores.keys():\n",
    "            x_data[count] =  re_features[i]\n",
    "            y_data[count] = scores[_id]\n",
    "            count = count + 1\n",
    "        pbar.update()\n",
    "\n",
    "    x_data = x_data[:count]\n",
    "    y_data = y_data[:count]\n",
    "    \n",
    "    # Write to the disk\n",
    "    bc_x_data = bc.carray(x_data, rootdir = x_path)\n",
    "    bc_x_data.flush()\n",
    "    with open(y_path, 'wb') as f:\n",
    "        pickle.dump(y_data, f)\n",
    "\n",
    "print(\"Shape of x data: \", x_data.shape, \" and shape of y_data: \", y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the normalized x data:  (3735, 50, 50, 50, 8)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(x_data, axis=0)\n",
    "std = np.std(x_data, axis=0)\n",
    "x_data = (x_data - mean)/(std + 0.000001)\n",
    "\n",
    "print(\"Shape of the normalized x data: \", x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids, test_ids = split_data(length=x_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any common ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_ids).intersection(set(valid_ids)).intersection(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size, mode='train'):\n",
    "    global x_data\n",
    "    global y_data\n",
    "    global train_ids\n",
    "    global valid_ids\n",
    "    \n",
    "    while True:\n",
    "        if mode == 'train':\n",
    "            rand_ids = np.random.choice(train_ids, batch_size)\n",
    "            sample_x, sample_y = x_data[rand_ids], y_data[rand_ids]\n",
    "        else:\n",
    "            rand_ids = np.random.choice(valid_ids, batch_size)\n",
    "            sample_x, sample_y = x_data[rand_ids], y_data[rand_ids]\n",
    "        \n",
    "        yield sample_x, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 19.2 ms, total: 33.7 ms\n",
      "Wall time: 32.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, 50, 50, 50, 8), (10,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's warm up the generator\n",
    "%time bx, by = next(generator(10), 'valid')\n",
    "bx.shape, by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequence(Sequence):\n",
    "    def __init__(self, batch_size):\n",
    "        global x_data_n\n",
    "        global y_data\n",
    "        self.X,self.y = x_data_n, y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.counter = 0\n",
    "        self.epoch_counter = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // self.batch_size\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "#         print(\"Batch #: \", str(idx))\n",
    "#         print(\"From \", str(idx*self.batch_size), \" to \", str((idx+1)*self.batch_size))\n",
    "        batch_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "#         print(\"Returned batch size: \", str(len(batch_x)))\n",
    "        self.counter += 1\n",
    "        \n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Method called at the end of every epoch.\n",
    "        \"\"\"\n",
    "#         print('\\nEpoch end: ' + str(self.epoch_counter) + ' counter: ' + str(self.counter))\n",
    "        self.epoch_counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(summary=False):\n",
    "    \"\"\" Return the Keras model of the network\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1st layer group\n",
    "    model.add(Conv3D(filters=32,\n",
    "                     kernel_size=(5, 5, 5),\n",
    "                     strides = (2, 2, 2),\n",
    "                     input_shape=(50, 50, 50, 8),\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     kernel_initializer=he_uniform(seed=9876),\n",
    "                     bias_initializer='zeros'))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           padding='valid'))\n",
    "    \n",
    "    # 2nd layer group\n",
    "    model.add(Conv3D(filters=64,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     padding='valid'))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           padding='valid'))\n",
    "    \n",
    "    # 3rd layer group\n",
    "    model.add(Conv3D(filters=128,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation='relu',\n",
    "                     padding='valid'))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "                           padding='valid'))\n",
    "    \n",
    "    # 4th layer group\n",
    "#     model.add(Conv3D(filters=128,\n",
    "#                      kernel_size=(3, 3, 3),\n",
    "#                      activation='relu',\n",
    "#                      padding='valid'))\n",
    "    \n",
    "#     model.add(MaxPooling3D(pool_size=(2, 2, 2),\n",
    "#                            #strides=(1, 1, 1),\n",
    "#                            padding='valid'))\n",
    "    \n",
    "    #model.add(AveragePooling3D(pool_size=(2, 2, 2), \n",
    "    #                           padding='valid'))\n",
    "                           \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 23, 23, 23, 32)    32032     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 11, 11, 11, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 9, 9, 9, 64)       55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 2, 2, 128)      221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 1, 1, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 375,265\n",
      "Trainable params: 375,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fec17e1f5c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model(summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gpus = 8\n",
    "nb_batch = nb_gpus*16\n",
    "nb_epochs = 50\n",
    "\n",
    "model = get_model()\n",
    "model = multi_gpu_model(model, gpus=nb_gpus)\n",
    "model.compile(optimizer=optimizers.adam(lr=0.0001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[metrics.mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'mean_squared_error']\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "print(model.metrics_names)\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_mean_squared_error:.2f}.hdf5\"\n",
    "# For multi_gpu_model, save_weights_only should be set to True. Otherwise, saving of the multi_gpu_model will cause an error\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_mean_squared_error',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gen = CustomSequence(batch_size=nb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = x_data[valid_ids], y_data[valid_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 18s 795ms/step - loss: 5.6610 - mean_squared_error: 5.6610 - val_loss: 4.1306 - val_mean_squared_error: 4.1306\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 4.13061, saving model to weights-improvement-01-4.13.hdf5\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 18s 790ms/step - loss: 4.4285 - mean_squared_error: 4.4285 - val_loss: 3.7226 - val_mean_squared_error: 3.7226\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 4.13061 to 3.72256, saving model to weights-improvement-02-3.72.hdf5\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 19s 808ms/step - loss: 4.2278 - mean_squared_error: 4.2278 - val_loss: 3.7273 - val_mean_squared_error: 3.7273\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 18s 762ms/step - loss: 3.8627 - mean_squared_error: 3.8627 - val_loss: 3.5536 - val_mean_squared_error: 3.5536\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 3.72256 to 3.55359, saving model to weights-improvement-04-3.55.hdf5\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 17s 750ms/step - loss: 4.0073 - mean_squared_error: 4.0073 - val_loss: 3.4299 - val_mean_squared_error: 3.4299\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 3.55359 to 3.42987, saving model to weights-improvement-05-3.43.hdf5\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 17s 750ms/step - loss: 3.6755 - mean_squared_error: 3.6755 - val_loss: 3.7771 - val_mean_squared_error: 3.7771\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 17s 742ms/step - loss: 3.4548 - mean_squared_error: 3.4548 - val_loss: 3.3743 - val_mean_squared_error: 3.3743\n",
      "\n",
      "Epoch 00007: val_mean_squared_error improved from 3.42987 to 3.37426, saving model to weights-improvement-07-3.37.hdf5\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 18s 783ms/step - loss: 3.3449 - mean_squared_error: 3.3449 - val_loss: 3.2458 - val_mean_squared_error: 3.2458\n",
      "\n",
      "Epoch 00008: val_mean_squared_error improved from 3.37426 to 3.24583, saving model to weights-improvement-08-3.25.hdf5\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 18s 762ms/step - loss: 3.5112 - mean_squared_error: 3.5112 - val_loss: 3.1943 - val_mean_squared_error: 3.1943\n",
      "\n",
      "Epoch 00009: val_mean_squared_error improved from 3.24583 to 3.19427, saving model to weights-improvement-09-3.19.hdf5\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 18s 771ms/step - loss: 3.3129 - mean_squared_error: 3.3129 - val_loss: 3.2279 - val_mean_squared_error: 3.2279\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 18s 766ms/step - loss: 3.0453 - mean_squared_error: 3.0453 - val_loss: 3.1765 - val_mean_squared_error: 3.1765\n",
      "\n",
      "Epoch 00011: val_mean_squared_error improved from 3.19427 to 3.17645, saving model to weights-improvement-11-3.18.hdf5\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 18s 762ms/step - loss: 3.0074 - mean_squared_error: 3.0074 - val_loss: 3.0859 - val_mean_squared_error: 3.0859\n",
      "\n",
      "Epoch 00012: val_mean_squared_error improved from 3.17645 to 3.08595, saving model to weights-improvement-12-3.09.hdf5\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 17s 761ms/step - loss: 3.0779 - mean_squared_error: 3.0779 - val_loss: 3.0902 - val_mean_squared_error: 3.0902\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 17s 761ms/step - loss: 3.0125 - mean_squared_error: 3.0125 - val_loss: 3.1489 - val_mean_squared_error: 3.1489\n",
      "\n",
      "Epoch 00014: val_mean_squared_error did not improve\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 17s 755ms/step - loss: 2.8925 - mean_squared_error: 2.8925 - val_loss: 3.2149 - val_mean_squared_error: 3.2149\n",
      "\n",
      "Epoch 00015: val_mean_squared_error did not improve\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 18s 777ms/step - loss: 2.7599 - mean_squared_error: 2.7599 - val_loss: 3.0774 - val_mean_squared_error: 3.0774\n",
      "\n",
      "Epoch 00016: val_mean_squared_error improved from 3.08595 to 3.07739, saving model to weights-improvement-16-3.08.hdf5\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 18s 772ms/step - loss: 2.7115 - mean_squared_error: 2.7115 - val_loss: 3.0401 - val_mean_squared_error: 3.0401\n",
      "\n",
      "Epoch 00017: val_mean_squared_error improved from 3.07739 to 3.04008, saving model to weights-improvement-17-3.04.hdf5\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 18s 772ms/step - loss: 2.7904 - mean_squared_error: 2.7904 - val_loss: 3.0313 - val_mean_squared_error: 3.0313\n",
      "\n",
      "Epoch 00018: val_mean_squared_error improved from 3.04008 to 3.03129, saving model to weights-improvement-18-3.03.hdf5\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 18s 789ms/step - loss: 2.6100 - mean_squared_error: 2.6100 - val_loss: 3.1749 - val_mean_squared_error: 3.1749\n",
      "\n",
      "Epoch 00019: val_mean_squared_error did not improve\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 18s 767ms/step - loss: 2.4813 - mean_squared_error: 2.4813 - val_loss: 3.3596 - val_mean_squared_error: 3.3596\n",
      "\n",
      "Epoch 00020: val_mean_squared_error did not improve\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 18s 763ms/step - loss: 2.5606 - mean_squared_error: 2.5606 - val_loss: 3.0771 - val_mean_squared_error: 3.0771\n",
      "\n",
      "Epoch 00021: val_mean_squared_error did not improve\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 18s 764ms/step - loss: 2.4716 - mean_squared_error: 2.4716 - val_loss: 3.0174 - val_mean_squared_error: 3.0174\n",
      "\n",
      "Epoch 00022: val_mean_squared_error improved from 3.03129 to 3.01739, saving model to weights-improvement-22-3.02.hdf5\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 18s 777ms/step - loss: 2.3193 - mean_squared_error: 2.3193 - val_loss: 3.1541 - val_mean_squared_error: 3.1541\n",
      "\n",
      "Epoch 00023: val_mean_squared_error did not improve\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 18s 765ms/step - loss: 2.4190 - mean_squared_error: 2.4190 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
      "\n",
      "Epoch 00024: val_mean_squared_error improved from 3.01739 to 3.00996, saving model to weights-improvement-24-3.01.hdf5\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 17s 757ms/step - loss: 2.3566 - mean_squared_error: 2.3566 - val_loss: 3.0011 - val_mean_squared_error: 3.0011\n",
      "\n",
      "Epoch 00025: val_mean_squared_error improved from 3.00996 to 3.00114, saving model to weights-improvement-25-3.00.hdf5\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 18s 775ms/step - loss: 2.2035 - mean_squared_error: 2.2035 - val_loss: 3.0044 - val_mean_squared_error: 3.0044\n",
      "\n",
      "Epoch 00026: val_mean_squared_error did not improve\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 17s 755ms/step - loss: 2.2098 - mean_squared_error: 2.2098 - val_loss: 3.4876 - val_mean_squared_error: 3.4876\n",
      "\n",
      "Epoch 00027: val_mean_squared_error did not improve\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 18s 773ms/step - loss: 1.9268 - mean_squared_error: 1.9268 - val_loss: 3.0748 - val_mean_squared_error: 3.0748\n",
      "\n",
      "Epoch 00028: val_mean_squared_error did not improve\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 18s 762ms/step - loss: 2.0759 - mean_squared_error: 2.0759 - val_loss: 2.9484 - val_mean_squared_error: 2.9484\n",
      "\n",
      "Epoch 00029: val_mean_squared_error improved from 3.00114 to 2.94838, saving model to weights-improvement-29-2.95.hdf5\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 18s 774ms/step - loss: 2.1576 - mean_squared_error: 2.1576 - val_loss: 3.1295 - val_mean_squared_error: 3.1295\n",
      "\n",
      "Epoch 00030: val_mean_squared_error did not improve\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 18s 787ms/step - loss: 1.9578 - mean_squared_error: 1.9578 - val_loss: 2.9460 - val_mean_squared_error: 2.9460\n",
      "\n",
      "Epoch 00031: val_mean_squared_error improved from 2.94838 to 2.94599, saving model to weights-improvement-31-2.95.hdf5\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 18s 769ms/step - loss: 2.0174 - mean_squared_error: 2.0174 - val_loss: 3.1462 - val_mean_squared_error: 3.1462\n",
      "\n",
      "Epoch 00032: val_mean_squared_error did not improve\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 18s 766ms/step - loss: 1.9535 - mean_squared_error: 1.9535 - val_loss: 3.0353 - val_mean_squared_error: 3.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mean_squared_error did not improve\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 18s 771ms/step - loss: 1.9906 - mean_squared_error: 1.9906 - val_loss: 2.9579 - val_mean_squared_error: 2.9579\n",
      "\n",
      "Epoch 00034: val_mean_squared_error did not improve\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 18s 782ms/step - loss: 1.8132 - mean_squared_error: 1.8132 - val_loss: 2.9372 - val_mean_squared_error: 2.9372\n",
      "\n",
      "Epoch 00035: val_mean_squared_error improved from 2.94599 to 2.93721, saving model to weights-improvement-35-2.94.hdf5\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 18s 782ms/step - loss: 1.8799 - mean_squared_error: 1.8799 - val_loss: 3.4608 - val_mean_squared_error: 3.4608\n",
      "\n",
      "Epoch 00036: val_mean_squared_error did not improve\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 18s 785ms/step - loss: 1.7720 - mean_squared_error: 1.7720 - val_loss: 2.9756 - val_mean_squared_error: 2.9756\n",
      "\n",
      "Epoch 00037: val_mean_squared_error did not improve\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 18s 771ms/step - loss: 1.7143 - mean_squared_error: 1.7143 - val_loss: 3.1047 - val_mean_squared_error: 3.1047\n",
      "\n",
      "Epoch 00038: val_mean_squared_error did not improve\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 18s 788ms/step - loss: 1.6323 - mean_squared_error: 1.6323 - val_loss: 3.2395 - val_mean_squared_error: 3.2395\n",
      "\n",
      "Epoch 00039: val_mean_squared_error did not improve\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 18s 768ms/step - loss: 1.6154 - mean_squared_error: 1.6154 - val_loss: 3.0026 - val_mean_squared_error: 3.0026\n",
      "\n",
      "Epoch 00040: val_mean_squared_error did not improve\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 18s 764ms/step - loss: 1.5206 - mean_squared_error: 1.5206 - val_loss: 3.1127 - val_mean_squared_error: 3.1127\n",
      "\n",
      "Epoch 00041: val_mean_squared_error did not improve\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 18s 773ms/step - loss: 1.4487 - mean_squared_error: 1.4487 - val_loss: 2.9704 - val_mean_squared_error: 2.9704\n",
      "\n",
      "Epoch 00042: val_mean_squared_error did not improve\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 17s 758ms/step - loss: 1.4705 - mean_squared_error: 1.4705 - val_loss: 2.9635 - val_mean_squared_error: 2.9635\n",
      "\n",
      "Epoch 00043: val_mean_squared_error did not improve\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 18s 764ms/step - loss: 1.4105 - mean_squared_error: 1.4105 - val_loss: 3.6471 - val_mean_squared_error: 3.6471\n",
      "\n",
      "Epoch 00044: val_mean_squared_error did not improve\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 18s 770ms/step - loss: 1.3867 - mean_squared_error: 1.3867 - val_loss: 3.1104 - val_mean_squared_error: 3.1104\n",
      "\n",
      "Epoch 00045: val_mean_squared_error did not improve\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 18s 762ms/step - loss: 1.2828 - mean_squared_error: 1.2828 - val_loss: 3.2576 - val_mean_squared_error: 3.2576\n",
      "\n",
      "Epoch 00046: val_mean_squared_error did not improve\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 17s 761ms/step - loss: 1.2759 - mean_squared_error: 1.2759 - val_loss: 3.1628 - val_mean_squared_error: 3.1628\n",
      "\n",
      "Epoch 00047: val_mean_squared_error did not improve\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 18s 768ms/step - loss: 1.2436 - mean_squared_error: 1.2436 - val_loss: 3.2630 - val_mean_squared_error: 3.2630\n",
      "\n",
      "Epoch 00048: val_mean_squared_error did not improve\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 18s 761ms/step - loss: 1.2178 - mean_squared_error: 1.2178 - val_loss: 3.3827 - val_mean_squared_error: 3.3827\n",
      "\n",
      "Epoch 00049: val_mean_squared_error did not improve\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 18s 765ms/step - loss: 1.1064 - mean_squared_error: 1.1064 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
      "\n",
      "Epoch 00050: val_mean_squared_error did not improve\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator(batch_size=nb_batch), validation_data=(valid_x, valid_y),\n",
    "                              epochs=nb_epochs,\n",
    "                              steps_per_epoch=len(train_ids)//nb_batch, \n",
    "                              callbacks=callbacks_list, \n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model and load the last saved weights\n",
    "\n",
    "model = get_model()\n",
    "model = multi_gpu_model(model, gpus=nb_gpus)\n",
    "model.load_weights(\"model-weights-35-2.94.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = x_data[test_ids], y_data[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4087525212512636"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(y_pred=pred, y_true=test_y)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
